{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d668f20-5e67-43fc-9a6a-9b0c25096ab6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1190d443-0be6-4ea7-8af2-c196aa015229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e8b50a-8669-406e-aebf-164ff8ccf180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62310</td>\n",
       "      <td>SberChallenge</td>\n",
       "      <td>В протерозойских отложениях органические остат...</td>\n",
       "      <td>чем представлены органические остатки?</td>\n",
       "      <td>{'text': ['известковыми выделениями сине-зелён...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28101</td>\n",
       "      <td>SberChallenge</td>\n",
       "      <td>В протерозойских отложениях органические остат...</td>\n",
       "      <td>что найдено в кремнистых сланцах железорудной ...</td>\n",
       "      <td>{'text': ['нитевидные водоросли, грибные нити'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48834</td>\n",
       "      <td>SberChallenge</td>\n",
       "      <td>В протерозойских отложениях органические остат...</td>\n",
       "      <td>что встречается в протерозойских отложениях?</td>\n",
       "      <td>{'text': ['органические остатки'], 'answer_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83056</td>\n",
       "      <td>SberChallenge</td>\n",
       "      <td>В протерозойских отложениях органические остат...</td>\n",
       "      <td>что относится к числу древнейших растительных ...</td>\n",
       "      <td>{'text': ['скопления графито-углистого веществ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5816</td>\n",
       "      <td>SberChallenge</td>\n",
       "      <td>В протерозойских отложениях органические остат...</td>\n",
       "      <td>как образовалось графито-углистое вещество?</td>\n",
       "      <td>{'text': ['в результате разложения Corycium en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id          title                                            context  \\\n",
       "0  62310  SberChallenge  В протерозойских отложениях органические остат...   \n",
       "1  28101  SberChallenge  В протерозойских отложениях органические остат...   \n",
       "2  48834  SberChallenge  В протерозойских отложениях органические остат...   \n",
       "3  83056  SberChallenge  В протерозойских отложениях органические остат...   \n",
       "4   5816  SberChallenge  В протерозойских отложениях органические остат...   \n",
       "\n",
       "                                            question  \\\n",
       "0             чем представлены органические остатки?   \n",
       "1  что найдено в кремнистых сланцах железорудной ...   \n",
       "2       что встречается в протерозойских отложениях?   \n",
       "3  что относится к числу древнейших растительных ...   \n",
       "4        как образовалось графито-углистое вещество?   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['известковыми выделениями сине-зелён...  \n",
       "1  {'text': ['нитевидные водоросли, грибные нити'...  \n",
       "2  {'text': ['органические остатки'], 'answer_sta...  \n",
       "3  {'text': ['скопления графито-углистого веществ...  \n",
       "4  {'text': ['в результате разложения Corycium en...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {\"train\": \"sberquad/train-00000-of-00001.parquet\", \"validation\": \"sberquad/validation-00000-of-00001.parquet\", \"test\": \"sberquad/test-00000-of-00001.parquet\"}\n",
    "df = pd.read_parquet(\"hf://datasets/kuznetsoffandrey/sberquad/\" + splits[\"train\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e99fbd-2fbb-4524-9d97-34c762e6716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"id\", \"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bfc012-bb17-4135-9d4b-411b0ccef4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45328, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edb6e50-5cc8-4360-8f6d-61f6e8248418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41444"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_idx = [i for i in range(len(df)) if df[\"answers\"][i][\"answer_start\"] != [-1] and \"\".join(df[\"answers\"][i][\"text\"]).strip() != \"\"]\n",
    "len(select_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd13638-668c-467a-b6e6-0ff8a8916ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41444, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[select_idx]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81db950e-1548-4a93-85df-b6edcc5b512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 7231, 756.3410867676865)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = df[\"context\"].map(len)\n",
    "lens.min(), lens.max(), lens.sum() / len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc14af65-1af1-4cea-94ba-af78a45e3c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install -U transformers torch torchvision torchaudio\n",
    "# !pip3 show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f0457b-d286-4861-8261-6c8c06557a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bd6817-b3d2-475f-8910-783500f718f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3622, 17025, 65361, 24094, 166, 102, 781, 40147, 8380, 1519, 2999, 67595, 65361, 24094, 17943, 24856, 16462, 128, 3622, 845, 30501, 860, 2999, 132, 9621, 17025, 4582, 49911, 37879, 3187, 49335, 130, 46577, 57539, 128, 29666, 3187, 63032, 128, 53445, 40732, 17914, 17995, 868, 1755, 132, 6776, 4582, 17090, 57539, 128, 861, 22579, 41664, 64678, 41845, 16340, 42085, 8067, 22343, 130, 13625, 41502, 20387, 128, 40281, 11157, 845, 6542, 56840, 20346, 33267, 12966, 10622, 114635, 52597, 9481, 132, 781, 87285, 34972, 884, 42584, 858, 20736, 106478, 1700, 64124, 18211, 21831, 25352, 59869, 73376, 128, 25766, 2059, 54248, 851, 14037, 128, 35637, 29542, 24955, 46588, 48833, 12668, 14064, 132, 781, 7162, 25484, 116843, 55214, 12819, 15222, 851, 20234, 18715, 7162, 23939, 24860, 55384, 41106, 132, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'offset_mapping': [(0, 0), (0, 3), (4, 16), (17, 29), (30, 37), (37, 38), (0, 0), (0, 1), (2, 7), (7, 10), (10, 12), (12, 16), (17, 27), (28, 40), (41, 48), (49, 60), (61, 68), (69, 73), (73, 74), (75, 78), (79, 80), (81, 85), (85, 86), (86, 90), (90, 91), (92, 95), (96, 108), (109, 115), (115, 121), (122, 131), (131, 133), (134, 138), (138, 139), (139, 146), (147, 157), (157, 158), (159, 163), (163, 165), (166, 172), (172, 173), (174, 183), (184, 189), (189, 192), (192, 196), (196, 197), (197, 200), (200, 201), (202, 207), (208, 214), (214, 219), (220, 230), (230, 231), (232, 233), (234, 239), (240, 250), (251, 263), (264, 272), (273, 282), (283, 292), (293, 297), (297, 300), (300, 301), (301, 304), (304, 310), (311, 319), (319, 320), (321, 331), (331, 336), (337, 338), (339, 349), (350, 360), (361, 364), (364, 366), (366, 369), (370, 372), (372, 375), (375, 379), (379, 381), (381, 382), (383, 384), (385, 391), (391, 394), (394, 395), (396, 399), (399, 400), (400, 403), (404, 413), (413, 416), (417, 425), (426, 432), (433, 440), (441, 444), (444, 451), (452, 461), (461, 462), (463, 467), (467, 470), (471, 475), (476, 477), (478, 483), (483, 484), (485, 492), (493, 504), (505, 508), (508, 512), (512, 515), (515, 518), (518, 521), (521, 522), (523, 524), (525, 530), (530, 535), (536, 541), (541, 545), (546, 554), (555, 562), (563, 564), (565, 571), (572, 582), (583, 588), (588, 593), (594, 602), (603, 620), (621, 629), (629, 630), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KirrAno93/rubert-base-cased-finetuned-squad\")\n",
    "max_length = 192\n",
    "tokenizer(df[\"question\"][0], df[\"context\"][0], max_length=max_length, truncation=\"only_second\", padding=\"max_length\", return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36dc3f74-8300-4a00-9242-7a33520ee9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3622, 17025, 65361, 24094, 166, 102, 781, 40147, 8380, 1519, 2999, 67595, 65361, 24094, 17943, 24856, 16462, 128, 3622, 845, 30501, 860, 2999, 132, 9621, 17025, 4582, 49911, 37879, 3187, 49335, 130, 46577, 57539, 128, 29666, 3187, 63032, 128, 53445, 40732, 17914, 17995, 868, 1755, 132, 6776, 4582, 17090, 57539, 128, 861, 22579, 41664, 64678, 41845, 16340, 42085, 8067, 22343, 130, 13625, 41502, 20387, 128, 40281, 11157, 845, 6542, 56840, 20346, 33267, 12966, 10622, 114635, 52597, 9481, 132, 781, 87285, 34972, 884, 42584, 858, 20736, 106478, 1700, 64124, 18211, 21831, 25352, 59869, 73376, 128, 25766, 2059, 54248, 851, 14037, 128, 35637, 29542, 24955, 46588, 48833, 12668, 14064, 132, 781, 7162, 25484, 116843, 55214, 12819, 15222, 851, 20234, 18715, 7162, 23939, 24860, 55384, 41106, 132, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'start_positions': 27, 'end_positions': 34}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preproc_func(row):\n",
    "    question = row.question.strip()\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        row.context,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answer = row.answers\n",
    "    start_pos, end_pos = 0, 0\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    \n",
    "    context_start = inputs[\"token_type_ids\"].index(1)\n",
    "    context_end = len(inputs[\"token_type_ids\"]) - 1\n",
    "    while inputs[\"token_type_ids\"][context_end] != 1:\n",
    "        context_end -= 1\n",
    "    context_end -= 1\n",
    "    \n",
    "    if not (offset_mapping[context_start][0] > end_char or offset_mapping[context_end][1] < start_char):\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset_mapping[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_pos = idx - 1\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset_mapping[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_pos = idx + 1\n",
    "\n",
    "    inputs[\"start_positions\"] = start_pos\n",
    "    inputs[\"end_positions\"] = end_pos\n",
    "    # print(tokenizer.decode(inputs[\"input_ids\"][context_start:context_end+1]), tokenizer.decode(inputs[\"input_ids\"][start_pos:end_pos+1]), answer, sep=\"\\n\")\n",
    "    return inputs\n",
    "\n",
    "preproc_func(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d811854-38e1-4433-9122-1a0d4a988e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abde450b-47e3-41a6-949e-b00c8dc0109d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35227, 3), (6217, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, shuffle=True, random_state=42)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae8cb6-e6d9-45fb-8cb4-17e93ad41d6f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea592dc-0087-451b-aaa1-11956334ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(3407)\n",
    "torch.cuda.manual_seed_all(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9dabc1-6a87-4ede-bb92-391540b408ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 195)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SberquadDs(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df.apply(preproc_func, axis=1).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.inputs[index][\"input_ids\"], dtype=torch.int32),\n",
    "            torch.tensor(self.inputs[index][\"attention_mask\"], dtype=torch.int8),\n",
    "            torch.tensor(self.inputs[index][\"start_positions\"], dtype=torch.int64),\n",
    "            torch.tensor(self.inputs[index][\"end_positions\"], dtype=torch.int64)\n",
    "        )\n",
    "        \n",
    "train_ds, test_ds = SberquadDs(train_df), SberquadDs(test_df)\n",
    "\n",
    "batch_size = 32\n",
    "train_dl, test_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4), DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "len(train_dl), len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90fba2a1-bbf6-4177-9037-d1e45e8288f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"KirrAno93/rubert-base-cased-finetuned-squad\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5c90164-4b8a-48cf-aed3-19acebeedb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, train_dl, optimizer, scheduler):\n",
    "    print(\"=\" * 8, \"Starting training\", \"=\" * 8)\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    for input_ids, attention_mask, start_pos, end_pos in tqdm(train_dl):\n",
    "        input_ids, attention_mask, start_pos, end_pos = input_ids.to(device), attention_mask.to(device), start_pos.to(device), end_pos.to(device)\n",
    "        outs = model(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(outs.start_logits, start_pos) + F.cross_entropy(outs.end_logits, end_pos)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3348f156-3e97-44ff-ba2b-19434f1428a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, val_dl):\n",
    "    print(\"=\" * 8, \"Starting validation\", \"=\" * 8)\n",
    "    model.eval()\n",
    "    val_loss, val_em, val_f1 = 0., 0., 0.\n",
    "    for input_ids, attention_mask, start_pos, end_pos in tqdm(val_dl):\n",
    "        input_ids, attention_mask, start_pos, end_pos = input_ids.to(device), attention_mask.to(device), start_pos.to(device), end_pos.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outs = model(input_ids, attention_mask)\n",
    "            loss = F.cross_entropy(outs.start_logits, start_pos) + F.cross_entropy(outs.end_logits, end_pos)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf42968-8829-45e5-b22c-2dbb0167e090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 ========\n",
      "======== Starting training ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b33e778b4540999be85df2fbf4a998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.634825864561464\n",
      "======== Starting validation ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173467bb2ad84f76a47b88b8c93bf9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 2.5087086365773126\n",
      "======== Epoch 2 ========\n",
      "======== Starting training ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71924b0a017c480a8cb92042ea4ce1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.8447496210521832\n",
      "======== Starting validation ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48188621a9424edcaef2ec5068c16886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 2.3141532039031003\n",
      "======== Epoch 3 ========\n",
      "======== Starting training ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6270313c18bf4b8dbe8b0bcf45af1397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.9101986776526466\n",
      "======== Starting validation ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dde90aee2245f892737050bfe336ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 2.72524761618712\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dl) * 0.5, num_training_steps=len(train_dl) * epochs)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"=\" * 8, f\"Epoch {epoch + 1}\", \"=\" * 8)\n",
    "\n",
    "    train_loss = train(model, train_dl, optimizer, scheduler)\n",
    "    print(f\"Train loss {train_loss}\")\n",
    "\n",
    "    val_loss = valid(model, test_dl)\n",
    "    print(f\"Validation loss {val_loss}\")\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20d12e82-1537-4a59-92e2-bf1714c013c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./flask_app/saved_model/\")\n",
    "tokenizer.save_pretrained(\"./flask_app/saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25bfcd74-660a-4fc1-b03f-e33922b5c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def normalization(text):\n",
    "    text = \" \".join(text.split()).lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "def exact_match(pred, true):\n",
    "    pred = np.vectorize(normalization)(pred)\n",
    "    true = np.vectorize(normalization)(true)\n",
    "    return np.mean(pred == true)\n",
    "\n",
    "def f1_score(pred, true):\n",
    "    pred = np.vectorize(normalization)(pred)\n",
    "    true = np.vectorize(normalization)(true)\n",
    "    res = 0.\n",
    "    for i in range(len(pred)):\n",
    "        pred_tokens = pred[i].split()\n",
    "        true_tokens = true[i].split()\n",
    "        common_tokens = set(pred_tokens) & set(true_tokens)\n",
    "        if len(common_tokens) != 0 and len(pred_tokens) != 0 and len(true_tokens) != 0:\n",
    "            prec = len(common_tokens) / len(pred_tokens)\n",
    "            rec = len(common_tokens) / len(true_tokens)\n",
    "            res += 2 * (prec * rec) / (prec + rec)\n",
    "    return res / len(pred)\n",
    "\n",
    "def get_answer(context, start, end):\n",
    "    return [context[i][start[i]:end[i]+1] for i in range(len(context))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85d8d374-56cf-4955-885a-ee825382efa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbef1d4338924050a5a272e8ba7094c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1.36262380809356 EM 0.5611289173789173 F1 0.7658744994034126\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "val_loss, val_em, val_f1 = 0., 0., 0.\n",
    "for input_ids, attention_mask, start_positions, end_positions in tqdm(test_dl):\n",
    "    input_ids, attention_mask, start_positions, end_positions = input_ids.to(device), attention_mask.to(device), start_positions.to(device), end_positions.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        start_pred = outputs.start_logits.argmax(axis=1).cpu().numpy()\n",
    "        end_pred = outputs.end_logits.argmax(axis=1).cpu().numpy()\n",
    "        start_true = start_positions.cpu().numpy()\n",
    "        end_true = end_positions.cpu().numpy()\n",
    "        input_ids = input_ids.cpu().tolist()\n",
    "        ans_pred = get_answer(input_ids, start_pred, end_pred)\n",
    "        ans_true = get_answer(input_ids, start_true, end_true)\n",
    "        text_pred = [tokenizer.decode(ans_pred[i]) for i in range(len(ans_pred))]\n",
    "        text_true = [tokenizer.decode(ans_true[i]) for i in range(len(ans_true))]\n",
    "        val_em += exact_match(text_pred, text_true)\n",
    "        val_f1 += f1_score(text_pred, text_true)\n",
    "\n",
    "val_loss /= len(test_dl)\n",
    "val_em /= len(test_dl)\n",
    "val_f1 /= len(test_dl)\n",
    "print(f\"Loss {val_loss} EM {val_em} F1 {val_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65e8a8b4-33ff-4be7-a6ab-cc8cebad1226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model/\", local_files_only=True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./saved_model/\", local_files_only=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60981159-530e-4b26-bbce-3bb5cb4ed7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question, context):\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        context,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d71d16-4776-44ce-8fba-e7bdfdd37227",
   "metadata": {},
   "source": [
    "### Test case 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54292b10-c40b-4581-978f-a932794d51d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в Древнем Египте'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Где встречаются первые упоминания о строении человеческого тела?\"\n",
    "context = \"Первые упоминания о строении человеческого тела встречаются в Древнем Египте...\"\n",
    "\n",
    "pred = predict(question, context)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544adb1-142f-47a8-9744-3fd5e7eb55a5",
   "metadata": {},
   "source": [
    "### Test case 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "727555a1-2cb4-4d16-99de-cb0147ca7776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Юрий Гагарин'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Кто совершил первый полет в космос?\"\n",
    "context = \"12 апреля 1961 года Юрий Гагарин стал первым человеком в мировой истории, совершившим полёт в космическое пространство. Ракета-носитель «Восток» с кораблём «Восток-1», на борту которого находился Гагарин, была запущена с космодрома Байконур, расположенного в Кызылординской области Казахской ССР. После 108 минут полёта Гагарин успешно приземлился в Саратовской области, неподалёку от Энгельса. 12 апреля 1961 года, день полёта Юрия Гагарина в космос, был объявлен праздником — Днём космонавтики.\"\n",
    "\n",
    "pred = predict(question, context)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bba8cb-f293-4c22-bcb6-53d58e89e250",
   "metadata": {},
   "source": [
    "### Test case 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5de7ac5-115a-478a-8ab0-6bf03d4d8d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'« Восток - 1 »'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Как назывался космический корабль?\"\n",
    "context = \"12 апреля 1961 года Юрий Гагарин стал первым человеком в мировой истории, совершившим полёт в космическое пространство. Ракета-носитель «Восток» с кораблём «Восток-1», на борту которого находился Гагарин, была запущена с космодрома Байконур, расположенного в Кызылординской области Казахской ССР. После 108 минут полёта Гагарин успешно приземлился в Саратовской области, неподалёку от Энгельса. 12 апреля 1961 года, день полёта Юрия Гагарина в космос, был объявлен праздником — Днём космонавтики.\"\n",
    "\n",
    "pred = predict(question, context)\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
